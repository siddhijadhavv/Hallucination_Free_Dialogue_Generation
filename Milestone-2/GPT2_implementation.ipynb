{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# import locale\n",
        "# locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "jk2wVXXthUxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "DdPUKg6A6_I1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-multilearn"
      ],
      "metadata": {
        "id": "q_gHtePKxbpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "JRdf04LwXA9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "BEPzRHHUhItl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00t8qXI-5FIz"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import string\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import torch\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report\n",
        "import requests\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
        "from sklearn.metrics import accuracy_score,hamming_loss\n",
        "from skmultilearn.problem_transform import BinaryRelevance, ClassifierChain, LabelPowerset\n",
        "import sklearn.metrics as skm\n",
        "from sklearn import metrics\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import shutil\n",
        "import sys   \n",
        "from transformers import BertTokenizer, BertModel, BertConfig, BertForSequenceClassification\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "pzbS43_fGFdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORVRVVfUsRTf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yf0Jj_fRkitD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZwJxPB9_ko7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4ZUDKZ55FI0"
      },
      "outputs": [],
      "source": [
        "nlp=spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_BU9cr65FI1"
      },
      "outputs": [],
      "source": [
        "nlp.pipe_names"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_convert_data(api_url):\n",
        "  get_data = requests.get(api_url)\n",
        "  data = get_data.json()\n",
        "  dialog_idx = []\n",
        "  response = []\n",
        "  original_response = []\n",
        "  history = []\n",
        "  knowledge = []\n",
        "  Begin = []\n",
        "  vrm = []\n",
        "  headers = []\n",
        "  for i in data[\"rows\"]:\n",
        "    for key,value in i.items():\n",
        "      if type(value)!=int and type(value)!=list:\n",
        "        for k,v in value.items():\n",
        "          if k not in headers:\n",
        "            headers.append(k)\n",
        "          if k == \"dialog_idx\":\n",
        "            dialog_idx.append(v)\n",
        "          if k == \"response\":\n",
        "            response.append(v)\n",
        "          if k == \"original_response\":\n",
        "            original_response.append(v)\n",
        "          if k == \"history\":\n",
        "            history.append(v)\n",
        "          if k == \"knowledge\":\n",
        "            knowledge.append(v)\n",
        "          if k == \"BEGIN\":\n",
        "            Begin.append(v)\n",
        "          if k == \"VRM\":\n",
        "            vrm.append(v)\n",
        "  full_data = list(zip(dialog_idx,response,original_response,history,knowledge,Begin,vrm))\n",
        "  testing_data = pd.DataFrame(full_data, columns=headers)\n",
        "  return testing_data"
      ],
      "metadata": {
        "id": "GOBeJKV-EEIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxG_ts1H1uBc"
      },
      "outputs": [],
      "source": [
        "def clean_data(data):\n",
        "   for i in range(0,len(data)):\n",
        "      #Convert text to lower\n",
        "      if type(data.iloc[i]) == float:\n",
        "         data.iloc[i]= str(data.iloc[i])\n",
        "      data.iloc[i] = data.iloc[i].lower()\n",
        "      #Tokenize the data using spacy\n",
        "      doc = nlp(data.iloc[i])\n",
        "      #Convert data to lower using spacy\n",
        "      tokens = [tokens.lower_ for tokens in doc]\n",
        "      #remove stop words\n",
        "      tokens = [tokens for tokens in doc if (tokens.is_stop == False)]\n",
        "      #remove Punctuation\n",
        "      tokens = [tokens for tokens in tokens if (tokens.is_punct == False)]\n",
        "      #lemmatize the data\n",
        "      final_token = [token.lemma_ for token in tokens]\n",
        "      #generate the cleaned text\n",
        "      data.iloc[i] = \" \".join(final_token)\n",
        "   return data\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def label_data(new_df, columns):\n",
        "  mlb = MultiLabelBinarizer()\n",
        "  mlb_df = mlb.fit_transform(new_df[columns].to_numpy())\n",
        "  df_ohe = pd.DataFrame(mlb_df,new_df.index, mlb.classes_)\n",
        "  final_df = pd.concat([new_df,df_ohe], axis=1)\n",
        "  return final_df"
      ],
      "metadata": {
        "id": "20EhRifYRYih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_duplicates(history):\n",
        "  blist = [j for i in history for j in i]\n",
        "  alist = [] \n",
        "  for i in blist:\n",
        "    if i not in alist:\n",
        "      alist.append(i)\n",
        "  return alist"
      ],
      "metadata": {
        "id": "a-Ha9cTAJ3jM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_seeker(dataset):\n",
        "  resp_hist = dataset[[\"response\", \"history\"]]\n",
        "  response = resp_hist[\"response\"].to_numpy()\n",
        "  history =  resp_hist[\"history\"].to_numpy()\n",
        "  alist = remove_duplicates(history)\n",
        "  seeker= [i for i in alist if i not in response]\n",
        "  df = pd.DataFrame({'seeker':seeker})\n",
        "  new_df = pd.concat([dataset, df],axis =1)\n",
        "  return new_df"
      ],
      "metadata": {
        "id": "DgJ_2NVHJs1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainUrl = read_convert_data(\"https://datasets-server.huggingface.co/first-rows?dataset=McGill-NLP%2FFaithDial&config=plain_text&split=validation\")\n",
        "train_seeker = generate_seeker(trainUrl)\n",
        "final_train_1 = label_data(train_seeker, \"BEGIN\")\n",
        "final_train = label_data(final_train_1, \"VRM\")\n",
        "final_train[:3]"
      ],
      "metadata": {
        "id": "H400HjYNK6_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testUrl=read_convert_data(\"https://datasets-server.huggingface.co/first-rows?dataset=McGill-NLP%2FFaithDial&config=plain_text&split=test\")\n",
        "test_seeker = generate_seeker(testUrl)\n",
        "final_test_1 = label_data(test_seeker, \"BEGIN\")\n",
        "final_test = label_data(final_test_1, \"VRM\")\n",
        "final_test[:3]"
      ],
      "metadata": {
        "id": "4NWOVCZ3T2_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_data(final_train[\"knowledge\"])\n",
        "clean_data(final_train[\"response\"])\n",
        "clean_data(final_train[\"seeker\"])"
      ],
      "metadata": {
        "id": "gZAXSFziU4uJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_data(final_test[\"knowledge\"])\n",
        "clean_data(final_test[\"response\"])\n",
        "clean_data(final_test[\"seeker\"])"
      ],
      "metadata": {
        "id": "2y66ExY2WzEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = final_train[[\"knowledge\", \"seeker\", \"response\"]]\n",
        "X_test = final_test[[\"knowledge\", \"seeker\", \"response\"]]\n",
        "y_train = final_train[[\"Entailment\",\"Generic\",\"Hallucination\",\"Uncooperative\"]]\n",
        "y_test = final_test[[\"Entailment\",\"Generic\",\"Hallucination\",\"Uncooperative\"]]"
      ],
      "metadata": {
        "id": "vaKGH2rXajTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pipeline\n",
        "model = BinaryRelevance(MultinomialNB())\n",
        "k_vect = TfidfVectorizer()\n",
        "s_vect = TfidfVectorizer()\n",
        "r_vect = TfidfVectorizer()\n",
        "c_transform = ColumnTransformer([('tfidf_k', k_vect, 'knowledge'),('tfidf_s', s_vect, 'seeker'),('tfidf_r', r_vect, 'response')], remainder='passthrough')\n",
        "pipe = Pipeline([('tfidf', c_transform),('classify', model)])\n",
        "pipe.fit(X_train,y_train)\n",
        "res = pipe.predict(X_test)"
      ],
      "metadata": {
        "id": "kTzc0UM6akPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report = classification_report(y_test , res, target_names=[\"Entailment\",\"Generic\",\"Hallucination\",\"Uncooperative\"])\n",
        "cnf_matrix = skm.multilabel_confusion_matrix(y_test, res)\n",
        "print(skm.classification_report(y_test,res))"
      ],
      "metadata": {
        "id": "xTJOnwEdlah6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "aMJ05JgxyOhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT IMPLEMENTATION\n",
        "---\n"
      ],
      "metadata": {
        "id": "km1p4ycih1JJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainUrl = read_convert_data(\"https://datasets-server.huggingface.co/first-rows?dataset=McGill-NLP%2FFaithDial&config=plain_text&split=validation\")\n",
        "train_seeker = generate_seeker(trainUrl)\n",
        "final_train_1 = label_data(train_seeker, \"BEGIN\")\n",
        "final_train = label_data(final_train_1, \"VRM\")\n",
        "final_train[:3]"
      ],
      "metadata": {
        "id": "jaO02gGo7z5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_list = ['Entailment', 'Generic',\t'Hallucination',\t'Uncooperative']"
      ],
      "metadata": {
        "id": "rcU2JUTkc0zM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "ZHRn_ZggkZ_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_train[\"CONTEXT\"] = final_train[\"knowledge\"] + \". \" + final_train[\"seeker\"] + \". \" + final_train[\"response\"]\n",
        "final_test[\"CONTEXT\"] = final_test[\"knowledge\"] + \". \" + final_test[\"seeker\"] + \". \" + final_test[\"response\"]\n"
      ],
      "metadata": {
        "id": "F6-sOrorFXw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_train_copy = final_train\n",
        "final_test_copy = final_test"
      ],
      "metadata": {
        "id": "CaRDnoTolfXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mp81nKm0BN6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_train.drop([\"dialog_idx\",\"response\",\"original_response\",\"history\",\"knowledge\",\"BEGIN\",\"VRM\",\"seeker\", \"Ack.\",\n",
        "       \"Advisement\", \"Disclosure\", \"Edification\", \"Question\"],axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "ZKBd9gwzlrm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_train.columns"
      ],
      "metadata": {
        "id": "FhY9rGg3BOpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_train = final_train[['CONTEXT', 'Entailment', 'Generic', 'Hallucination', 'Uncooperative']]"
      ],
      "metadata": {
        "id": "_LD6AC6CClDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 256\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "VALID_BATCH_SIZE = 32\n",
        "EPOCHS = 4\n",
        "LEARNING_RATE = 1e-05"
      ],
      "metadata": {
        "id": "njC95AVoVpjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.df = df\n",
        "        self.title = df['CONTEXT']\n",
        "        self.targets = self.df[target_list].values\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.title)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        title = str(self.title[index])\n",
        "        title = \" \".join(title.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            title,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
        "            'targets': torch.FloatTensor(self.targets[index])\n",
        "        }\n",
        "     \n"
      ],
      "metadata": {
        "id": "st6pLcMaJV37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = CustomDataset(final_train, tokenizer, MAX_LEN)\n",
        "test_ds = CustomDataset(final_test, tokenizer, MAX_LEN)"
      ],
      "metadata": {
        "id": "OBrcA4hVbfc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_loader = torch.utils.data.DataLoader(train_ds, \n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(test_ds, \n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ],
      "metadata": {
        "id": "L-O96E4RdJgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_loader"
      ],
      "metadata": {
        "id": "hxoD94-ZyImO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "id": "F5XXQKI7dXER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "    \"\"\"\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    model: model that we want to load checkpoint parameters into       \n",
        "    optimizer: optimizer we defined in previous training\n",
        "    \"\"\"\n",
        "    # load check point\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    # initialize state_dict from checkpoint to model\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    # initialize optimizer from checkpoint to optimizer\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
        "    valid_loss_min = checkpoint['valid_loss_min']\n",
        "    # return model, optimizer, epoch value, min validation loss \n",
        "    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()\n",
        "\n",
        "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
        "    \"\"\"\n",
        "    state: checkpoint we want to save\n",
        "    is_best: is this the best checkpoint; min validation loss\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    best_model_path: path to save best model\n",
        "    \"\"\"\n",
        "    f_path = checkpoint_path\n",
        "    # save checkpoint data to the path given, checkpoint_path\n",
        "    torch.save(state, f_path)\n",
        "    # if it is a best model, min validation loss\n",
        "    if is_best:\n",
        "        best_fpath = best_model_path\n",
        "        # copy that checkpoint file to best path given, best_model_path\n",
        "        shutil.copyfile(f_path, best_fpath)"
      ],
      "metadata": {
        "id": "J0zZ_Z3vDFES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTClass, self).__init__()\n",
        "        self.bert_model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.linear = torch.nn.Linear(768, 4)\n",
        "    \n",
        "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
        "        output = self.bert_model(\n",
        "            input_ids, \n",
        "            attention_mask=attn_mask, \n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "        output_dropout = self.dropout(output.pooler_output)\n",
        "        output = self.linear(output_dropout)\n",
        "        return output\n",
        "\n",
        "model = BERTClass()\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "MSV7YeEqfoNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
        "\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "tadyDqgODK-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_targets=[]\n",
        "val_outputs=[]"
      ],
      "metadata": {
        "id": "1RdBxhpqDOYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(n_epochs, training_loader, validation_loader, model, \n",
        "                optimizer, checkpoint_path, best_model_path):\n",
        "   \n",
        "  # initialize tracker for minimum validation loss\n",
        "  valid_loss_min = np.Inf\n",
        "  for epoch in range(1, n_epochs+1):\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    print('############# Epoch {}: Training Start   #############'.format(epoch))\n",
        "    for batch_idx, data in enumerate(training_loader):\n",
        "        #print('yyy epoch', batch_idx)\n",
        "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        #if batch_idx%5000==0:\n",
        "         #   print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #print('before loss data in training', loss.item(), train_loss)\n",
        "        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n",
        "        #print('after loss data in training', loss.item(), train_loss)\n",
        "    \n",
        "    print('############# Epoch {}: Training End     #############'.format(epoch))\n",
        "    \n",
        "    print('############# Epoch {}: Validation Start   #############'.format(epoch))\n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        " \n",
        "    model.eval()\n",
        "   \n",
        "    with torch.no_grad():\n",
        "      for batch_idx, data in enumerate(validation_loader, 0):\n",
        "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n",
        "            val_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "\n",
        "      print('############# Epoch {}: Validation End     #############'.format(epoch))\n",
        "      # calculate average losses\n",
        "      #print('before cal avg train loss', train_loss)\n",
        "      train_loss = train_loss/len(training_loader)\n",
        "      valid_loss = valid_loss/len(validation_loader)\n",
        "      # print training/validation statistics \n",
        "      print('Epoch: {} \\tAvgerage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n",
        "            epoch, \n",
        "            train_loss,\n",
        "            valid_loss\n",
        "            ))\n",
        "      \n",
        "      # create checkpoint variable and add important data\n",
        "      checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'valid_loss_min': valid_loss,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "      }\n",
        "        \n",
        "        # save checkpoint\n",
        "      save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n",
        "        \n",
        "      ## TODO: save the model if validation loss has decreased\n",
        "      if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
        "        # save checkpoint as best model\n",
        "        save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
        "        valid_loss_min = valid_loss\n",
        "\n",
        "    print('############# Epoch {}  Done   #############\\n'.format(epoch))\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "63N6tdIcDVLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_path = \"/content/curr_ckpt\"\n",
        "best_model_path = \"/content/best_model.pt\""
      ],
      "metadata": {
        "id": "YVvK74WDDYIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = train_model(EPOCHS, train_data_loader, val_data_loader, model, optimizer,ckpt_path, best_model_path)"
      ],
      "metadata": {
        "id": "NpgsKGKSDhLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "5QB8Cgf-b6nI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_Url = read_convert_data(\"https://datasets-server.huggingface.co/first-rows?dataset=McGill-NLP%2FFaithDial&config=plain_text&split=validation\")\n",
        "train_seeker_1 = generate_seeker(text_Url)\n",
        "final_train_1 = label_data(train_seeker_1, \"BEGIN\")\n",
        "final_TEST = label_data(final_train_1, \"VRM\")\n",
        "final_TEST[10:16]"
      ],
      "metadata": {
        "id": "31D2W7H2ijM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QHjj-NJqRPF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation():\n",
        "    model.eval()\n",
        "    fin_targets=[]\n",
        "    fin_outputs=[]\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(val_data_loader, 0):\n",
        "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "    return fin_outputs, fin_targets"
      ],
      "metadata": {
        "id": "QWWkzelQkhG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs, targets = validation()\n",
        "outputs = np.array(outputs) >= 0.5\n",
        "accuracy = skm.accuracy_score(targets, outputs)\n",
        "f1_score_micro = skm.f1_score(targets, outputs, average='micro')\n",
        "f1_score_macro = skm.f1_score(targets, outputs, average='macro')\n",
        "print(f\"Accuracy Score = {accuracy}\")\n",
        "print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
        "print(f\"F1 Score (Macro) = {f1_score_macro}\")"
      ],
      "metadata": {
        "id": "P13h3mddkAhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Generation**"
      ],
      "metadata": {
        "id": "TXwdzEI0Chz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install huggingface_hub"
      ],
      "metadata": {
        "id": "nfFo8CBXwxK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "gtBBFmRAJlhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer_qa = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "lBspNRhmC0OA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainUrl1 = read_convert_data(\"https://datasets-server.huggingface.co/first-rows?dataset=McGill-NLP%2FFaithDial&config=plain_text&split=validation\")\n",
        "train_seeker1 = generate_seeker(trainUrl1)\n",
        "final_train_11 = label_data(train_seeker1, \"BEGIN\")\n",
        "final_train1 = label_data(final_train_11, \"VRM\")\n",
        "final_train1[:3]"
      ],
      "metadata": {
        "id": "DFn01PFsC44M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testUrl1 =read_convert_data(\"https://datasets-server.huggingface.co/first-rows?dataset=McGill-NLP%2FFaithDial&config=plain_text&split=test\")\n",
        "test_seeker1 = generate_seeker(testUrl1)\n",
        "final_test_11 = label_data(test_seeker1, \"BEGIN\")\n",
        "final_test1 = label_data(final_test_11, \"VRM\")\n",
        "final_test1[:3]"
      ],
      "metadata": {
        "id": "KpoqWTQsQ6L9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict"
      ],
      "metadata": {
        "id": "S_Ugh7vLeshq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = Dataset.from_pandas(final_train1)\n",
        "test_df = Dataset.from_pandas(final_test1)\n",
        "\n",
        "full_data = DatasetDict()\n",
        "full_data['train'] = train_df\n",
        "full_data['test'] = test_df"
      ],
      "metadata": {
        "id": "L-T48zEYNMp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    seekers = [q.strip() for q in examples[\"seeker\"]]\n",
        "    inputs = tokenizer(\n",
        "        seekers,\n",
        "        examples[\"knowledge\"],\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    responses = examples[\"response\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        response = responses[i]\n",
        "        start_char = 0\n",
        "        end_char = len(response)\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label it (0, 0)\n",
        "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "XMIIVY0TEvqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_squad = full_data.map(preprocess_function, batched=True, remove_columns=full_data[\"train\"].column_names)"
      ],
      "metadata": {
        "id": "C78X2EHpERBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DefaultDataCollator\n",
        "\n",
        "data_collator = DefaultDataCollator()"
      ],
      "metadata": {
        "id": "c2eUPyf5Y_c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "_4HGbSr-ER_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"my_qa_model\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_squad[\"train\"],\n",
        "    eval_dataset=tokenized_squad[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(\"my_qa_model\")"
      ],
      "metadata": {
        "id": "Yi6VHGz_ZC_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import create_optimizer\n",
        "\n",
        "batch_size = 16\n",
        "num_epochs = 2\n",
        "total_train_steps = (len(tokenized_squad[\"train\"]) // batch_size) * num_epochs\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=2e-5,\n",
        "    num_warmup_steps=0,\n",
        "    num_train_steps=total_train_steps,\n",
        ")"
      ],
      "metadata": {
        "id": "sxaDpcspBfsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelForQuestionAnswering\n",
        "\n",
        "model = TFAutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "LQ8XDhaQgoA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_train_set = model.prepare_tf_dataset(\n",
        "    tokenized_squad[\"train\"],\n",
        "    shuffle=True,\n",
        "    batch_size=4,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "tf_validation_set = model.prepare_tf_dataset(\n",
        "    tokenized_squad[\"test\"],\n",
        "    shuffle=False,\n",
        "    batch_size=4,\n",
        "    collate_fn=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "39Q0bOlMgqm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model.compile(optimizer=optimizer)"
      ],
      "metadata": {
        "id": "Qx_47kbu6yJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y9XRQ_81DCk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "\n",
        "callback = PushToHubCallback(\n",
        "    output_dir=\"my_awesome_qa_model\",\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "Uq1CbPM760yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=2)"
      ],
      "metadata": {
        "id": "BRL-D0u76-ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uZwABDYjDGhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How many programming languages does BLOOM support?\"\n",
        "context = \"BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages.\""
      ],
      "metadata": {
        "id": "dIGBlCQz7BjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\", model=\"my_awesome_qa_model\")\n",
        "question_answerer(question=question, context=context)"
      ],
      "metadata": {
        "id": "_dQXb8V97GNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"my_awesome_qa_model\")\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)"
      ],
      "metadata": {
        "id": "zkNW3Jz47J_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_start_index = outputs.start_logits.argmax()\n",
        "answer_end_index = outputs.end_logits.argmax()"
      ],
      "metadata": {
        "id": "MyV9uofuCoEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "tokenizer.decode(predict_answer_tokens)"
      ],
      "metadata": {
        "id": "oK2C6SQHCp5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rJ-eCK00CtQi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}